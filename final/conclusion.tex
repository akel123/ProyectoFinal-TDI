\section{Conclusion}
We learned KL-divergence as a way to extend the concept of Shannon entropy. To expand this idea even further, we utilize a continuous homogeneous function $\phi(x,y)$ convex in both arguments satisfying $\phi(1,1)=0$. Then the calculation is simple, $\sum_i \phi(x_i, y_i)$. The idea is that $\phi(x_i, y_i)$ tells us something meaningful about the difference between each pair of elements in the probability vectors $P$ and $Q$. If we think about the function $H_\phi$ in the equivalent sense, $H_g = \sum_i x_i g\left(\frac{y_i}{x_i} - 1\right)$, the relative entropy is really just an average with respect to $P$, weighted by $g\left(\frac{y_i}{x_i}-1\right)$. Properties of $\phi$ such as continuity, homogeneity, and convexity alows for many of the properties of the classical relative entropy still hold under this generalization, which allows for a meaningful contraction coefficient, $\eta_\phi$. This allows us to speak of data processing inequalities with respect to different measures of relative entropy. In this paper, we were able to set upper and lower bounds on $\eta_\phi$ with respect to a stochastic matrix $A$. One can imagine that with an upper and lower bound on the contraction coefficient, the effect of $A$ on probability vectors $x,y$ can be assessed in much a more meaninful fashion. One example would be bounding the rates of convergence to equilibrium of Markov processes, as mentioned in the introduction. 


