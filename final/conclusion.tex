\section{Conclusion}
We can think of relative entropy from $Q$ to $P$ (or $KL$ Divergence) as the \textit{information gain}. That is, we have a random variable $X$ we are using $Q$ as it's distribution. The relative entropy tells us how much information we could gain about $X$ if we were to use $P$ instead. To expand this idea into a more abstract sense, we need to define a continuous homogeneous function $\phi(x,y)$ convex in both arguments satisfying $\phi(1,1)=0$.  Then the calculation is simple, $\sum_i \phi(x_i, y_i)$. The idea is that $\phi(x_i, y_i)$ tells us something meaningful about the difference between each pair of elements. $\phi$ is defined as such so that many of the same properties of the classical relative entropy still hold. For example Using $g(t) \equiv \phi(1, 1+t)$ to index the relative $\phi$-entropy facilitated many of the proofs and lines of thought presented in this paper. Defining and elaborating the contraction coefficient $\eta_g(A)$ gives many usefull inequalities in studing the effects of the (stochastic) matrix $A$ on probability vectores $x,y$ through the lense of relative entropy.

