\section{Preliminares}
\begin{itemize}
    \item give the definitions of the different types of random matricies
    \begin{itemize}
        \item positive and nonnegative probability $d$-vectors
        \item row/column stochastic matrix
        \item scrambling matrix
    \end{itemize}
    \item And I think we need the following definitions to give context for the contraction coefficient
    \begin{definition*}[2.1 $\to$ Symmetric Relative Entropy]
        
    \end{definition*}
    \begin{definition*}[2.2 $\to$ relative $\phi$-entropy]
        
    \end{definition*}
    \begin{definition*}[2.3]
        Para cualquier $m \times n$ matriz $A$, el coeficiente de ergodicity de Dobrushin se define
        \[\alpha(A) = \min_{j, k}\sum_{i = 1}^m \min(a_{ij},a_{ik})\]
    \end{definition*}
    The complement $1- \alpha(A)$ is 
    \[\bar{\alpha}(A) \equiv 1 - \alpha(A) = \frac{1}{2} \max_{j,k}\sum_{i = 1}^m|a_{ij} - a_{ik}|\]
    and also satisfies
    \[\bar{\alpha}(A) = \sup \left\{\frac{\norm{A(x-y)}_1}{\norm{x-y}_1}: \text{ $x$ and $y$ are positive $n$-vectors such that } x\neq y, \, \text{y} \, \norm{x}_1 = \norm{y}_1\right\}\]
    % \begin{definition*}[Relative $\mathbf{g}$-Entropy]
    %     Sean \[x = (x_1, \dots, x_n) \quad y = (y_1, \dots, y_n)\] dos distribuciones de probabilidad. Se define la \textit{$g$-entrop√≠a relativa}
    % \[H_g(x,y) = \sum_k x_k \, g\!\left(\frac{y_k}{x_k} - 1\right)\]
    % \end{definition*}
    \item introduce $g(t) = \phi(1, 1+t)$ as a way to index the contraction coefficients
\end{itemize}



