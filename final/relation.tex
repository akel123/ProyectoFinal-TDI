\section{Relation to Course}
In the course, we first defined and studied the properties of $KL$-divergence (in fact, this is the relative entropy defined in Definition 2.1, also called $H_{\log}$ throughout the paper), and generalized it to a family of divergences called $f$-divergence. This paper also forms a generalization of $KL$-divergence, but instead derives a more abstract version of relative entropy. These two methods of generalization can be connected, and we'll see that relative entropy is a type of $f$-divergence. 

Let $f: (0, \infty) \to \R$ be convex and $f(1) = 0$. Furthermore, let $P$ and $Q$ be probability distributions such that $P <\!< Q$. Define $f$-divergence to be 
\[D_f(P || Q):= \int f\left(\frac{dP}{dQ}\right)dQ \xrightarrow{\text{discrete}} \sum_i f\left(\frac{p_i}{q_i}\right)q_i\]
where $p = (p_i)$ and $q = (q_i)$ are two $d$-vectors. Thus $f$-divergence (in the discrete case) is equivalente to the relative $g$-entropy if and only if $f(z) = z g\left(\frac{1}{z}-1\right)$ \footnote{Note that $g$ is not necessarily invertible}. Then \[D_f(P|| Q) = \sum_i f\left(\frac{p_i}{q_i}\right)q_i = \sum_i \frac{p_i}{\cancel{q_i}}\cancel{q_i}g\left(\frac{1}{p_i/q_i}\right)=\sum_i p_i g\left(\frac{q_i}{p_i} - 1\right) = H_g(P, Q)\]
slightly abusing notation when we think about $P$ and $Q$ as vectors of their respective probabilities indexed at $i$. For $z \in (0, \infty)$, $\frac{1}{z}-1 \in (-1, \infty)$ so we have that $z \mapsto zg\left(\frac{1}{z}-1\right)$ is convex. And of course, $f(1) = 1g(1 - 1) = g(0) = 0$; thus, everything is well defined.

On a related note, we can also extend $f$ divergence to be symmetric in a similar way to how the symmetric relative entropy is defined, $J_{f}(x,y) = D_f(x,y) + D_f(y,x)$. Note that this still does not permit us to think about divergence as a metric, as it still does not satisfy the triangle inequality.

Lastly, we can draw some parallels about the Data Processing Inequality that we've seen in class. $A$ is a scrambling matrix if and only if $\eta_{\phi}(A) < 1$; furthermore, we have that $n_\phi(A)=1$ if $A$ is a permutation matrix. Clearly, by definition of $\eta_\phi(A)$ we have that $H_{\phi}(Ax,Ay)  \leq H_{\phi}(x,y)$ with equality if and only if $A$ is a permutation matrix. In class we saw the DPI in the context of divergence,\[D(P_{Y|X}P_X||P_{Y|X}Q_X) \leq D(P_x||Q_x)\] Because of how the contraction coefficient was defined in this paper, we can easily set $P_{X|Y} = A$ and $P_{Y|Z} = A$ and set up the markov chain such that 
\[X \xrightarrow{A} Y \xrightarrow{A^\prime} Z \implies H_{\phi}(X,Y) \leq H_{\phi}(X,Z)\]
where the equality only holds if both $A$ and $A^\prime$ are permutation matrices.

    