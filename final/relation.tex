\section{Relation to Course}
\begin{itemize}
    % \item convexity
    % \begin{itemize}
    %     \item Contraction coefficient depends on $\phi(1, \cdot)$ being strictly convex on $(0, \infty)$
    % \end{itemize}
    \item compare with what we've seen with DPI from $KL$-divergence and mutual information. \\ Briefly, we have
    % \[H_\phi (Ax,Ay) \leq \bar{\alpha}(A)H_{\phi}(x,y)\]
    % and
    \[\eta_{\phi}(A) = \sup \left\{\frac{H_{\phi}(Ax,Ay)}{H_{\phi}(x,y)}: x\in P_n, y\in P_n, x \neq y\right\}\]
    and also
    \begin{align*}
        A \text{ is scrambling } &\iff \eta_{\phi}(A) < 1 \\
        &\iff H_{\phi}(x,y) < H_{\phi}(Ax,Ay) &&\text{with equality if $A$ is permutation}
    \end{align*}
    So 
    \[X \xrightarrow{A} Y \xrightarrow{A^\prime} Z \implies H_{\phi}(X,Y) < H_{\phi}(X,Z)\]
    
    Seems like this connects easily to what we've seen if we define $P_{X|Y} \equiv A$ and $P_{Y|Z} \equiv A^\prime$
    \item Compare relative $g$-entropy with $f$-divergence
    \begin{itemize}
        \item Extend symmetry to $f$-divergence
        \[J_{\phi}(x,y) = H_{\phi}(x,y) + H_{\phi}(y,x) \longrightarrow J_{f}(x,y) = D_f(x,y) + H_f(y,x)\]
    \end{itemize}
\end{itemize}